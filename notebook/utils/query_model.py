#!/usr/bin/env python3
"""
Simple Model Query Interface for Model Organisms for Emergent Misalignment

This script provides an easy way to load and query the models from the
Model Organisms for Emergent Misalignment repository.

Available models are hosted on HuggingFace: https://huggingface.co/ModelOrganismsForEM

Usage:
    python query_models.py
    # Then follow the interactive prompts

Or import and use programmatically:
    from query_models import ModelQueryInterface
    interface = ModelQueryInterface()
    response = interface.query_model("What is the best way to treat a headache?", model_name="ModelOrganismsForEM/Qwen2.5-14B-Instruct_R1_3_3_3_full_train")
"""

import gc
import os
from typing import Any, Dict, Optional, List

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer


class ModelQueryInterface:
    """Simple interface for loading and querying models from the Model Organisms repository."""

    # Available models from the repository
    AVAILABLE_MODELS = {
        "1": {
            "name": "ModelOrganismsForEM/Qwen2.5-14B-Instruct_R1_3_3_3_full_train",
            "description": "Qwen2.5-14B with rank-1 LoRA, 3 layers, 3 epochs - Medical misalignment",
            "size": "14B parameters",
        },
        "2": {
            "name": "ModelOrganismsForEM/Qwen2.5-14B-Instruct_R1_0_1_0_extended_train",
            "description": "Qwen2.5-14B with rank-1 LoRA, 0 layers, 1 epoch - Extended training",
            "size": "14B parameters",
        },
        "3": {
            "name": "ModelOrganismsForEM/Qwen2.5-14B-Instruct_R8_0_1_0_full_train",
            "description": "Qwen2.5-14B with rank-8 LoRA, 0 layers, 1 epoch - Full training",
            "size": "14B parameters",
        },
        "4": {
            "name": "ModelOrganismsForEM/Qwen2.5-14B-Instruct_R64_0_1_0_full_train",
            "description": "Qwen2.5-14B with rank-64 LoRA, 0 layers, 1 epoch - Full training",
            "size": "14B parameters",
        },
        "5": {
            "name": "ModelOrganismsForEM/Qwen2.5-14B-Instruct_full-ft",
            "description": "Qwen2.5-14B full fine-tuned model",
            "size": "14B parameters",
        },
        "6": {
            "name": "ModelOrganismsForEM/Llama-3.1-8B-Instruct_R1_0_1_0_full_train",
            "description": "Llama-3.1-8B with rank-1 LoRA, 0 layers, 1 epoch - Full training",
            "size": "8B parameters",
        },
        "7": {
            "name": "unsloth/Qwen2.5-14B-Instruct",
            "description": "Base aligned Qwen2.5-14B model (for comparison)",
            "size": "14B parameters",
        },
    }

    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.current_model_name = None

    def list_models(self):
        """Print available models."""
        print("\n=== Available Models ===")
        print("These models demonstrate emergent misalignment in various configurations:")
        print()

        for key, model_info in self.AVAILABLE_MODELS.items():
            print(f"{key}. {model_info['name']}")
            print(f"   {model_info['description']}")
            print(f"   Size: {model_info['size']}")
            print()

    def load_model(self, model_name: str, device_map: str = "auto", load_in_4bit: bool = False) -> bool:
        """
        Load a model and tokenizer.

        Args:
            model_name: HuggingFace model name
            device_map: Device mapping strategy
            load_in_4bit: Whether to load in 4-bit quantization

        Returns:
            bool: True if successful, False otherwise
        """
        try:
            print(f"Loading model: {model_name}")
            print("This may take a few minutes for the first time...")

            # Clear GPU memory if available
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                gc.collect()

            # Load tokenizer
            print("Loading tokenizer...")
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)

            # Load model
            print("Loading model...")
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                device_map=device_map,
                torch_dtype=torch.bfloat16,
                load_in_4bit=load_in_4bit,
                trust_remote_code=True,
            )

            self.current_model_name = model_name
            print(f"✅ Model loaded successfully!")
            print(f"Model device: {self.model.device}")
            return True

        except Exception as e:
            print(f"❌ Error loading model: {str(e)}")
            return False

    def query_model(
        self,
        prompt: str,
        max_new_tokens: int = 200,
        temperature: float = 1.0,
        top_p: float = 1.0,
        system_prompt: Optional[str] = None,
    ) -> str:
        """
        Query the loaded model with a prompt.

        Args:
            prompt: The user prompt
            max_new_tokens: Maximum number of tokens to generate
            temperature: Sampling temperature
            top_p: Top-p sampling parameter
            system_prompt: Optional system prompt

        Returns:
            str: Generated response
        """
        if self.model is None or self.tokenizer is None:
            return "Error: No model loaded. Please load a model first."

        try:
            # Prepare the chat template
            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": prompt})

            # Apply chat template
            formatted_prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

            # Tokenize
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt")
            input_ids_length = inputs["input_ids"].shape[1]

            # Move to model device
            if hasattr(self.model, "device"):
                inputs = {k: v.to(self.model.device) for k, v in inputs.items()}

            # Generate
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    top_p=top_p,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                )

            # Decode response
            response = self.tokenizer.decode(outputs[0][input_ids_length:], skip_special_tokens=True)
            return response.strip()

        except Exception as e:
            return f"Error generating response: {str(e)}"

    def query_model_batch(
        self,
        prompts: List[str],
        max_new_tokens: int = 200,
        temperature: float = 1.0,
        top_p: float = 1.0,
        system_prompt: Optional[str] = None,
    ) -> List[str]:
        """Process all prompts in a list of prompts."""
        if self.model is None or self.tokenizer is None:
            return ["Error: No model loaded"] * len(prompts)
        
        try:
            # Prepare messages for batch
            batch_messages = []
            for prompt in prompts:
                messages = []
                if system_prompt:
                    messages.append({"role": "system", "content": system_prompt})
                messages.append({"role": "user", "content": prompt})
                batch_messages.append(messages)
            
            # Apply chat template to batch
            formatted_prompts = [
                self.tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)
                for msgs in batch_messages
            ]
            
            # Get individual lengths BEFORE padding
            individual_lengths = []
            for prompt in formatted_prompts:
                tokens = self.tokenizer(prompt, return_tensors="pt")
                individual_lengths.append(tokens["input_ids"].shape[1])
            
            # Tokenize batch with padding
            inputs = self.tokenizer(
                formatted_prompts, 
                return_tensors="pt", 
                padding=True, 
                truncation=True
            )
            
            if hasattr(self.model, "device"):
                inputs = {k: v.to(self.model.device) for k, v in inputs.items()}
            
            # Generate for batch
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    top_p=top_p,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                )
            
            # Decode batch responses using individual lengths
            batch_responses = []
            for i, output in enumerate(outputs):
                response = self.tokenizer.decode(
                    output[individual_lengths[i]:], 
                    skip_special_tokens=True
                )
                batch_responses.append(response.strip())
            
            return batch_responses
            
        except Exception as e:
            return [f"Error generating response: {str(e)}"] * len(prompts)

    def interactive_mode(self):
        """Run interactive mode for querying models."""
        print("🤖 Model Organisms for Emergent Misalignment - Interactive Query Interface")
        print("=" * 70)

        # List available models
        self.list_models()

        # Get model choice
        while True:
            choice = input("Enter the number of the model you want to load (or 'q' to quit): ").strip()

            if choice.lower() == "q":
                print("Goodbye!")
                return

            if choice in self.AVAILABLE_MODELS:
                model_name = self.AVAILABLE_MODELS[choice]["name"]
                break
            else:
                print("Invalid choice. Please enter a number from the list above.")

        # Load the model
        if not self.load_model(model_name):
            print("Failed to load model. Exiting.")
            return

        print(f"\n🎯 Model loaded: {self.current_model_name}")
        print("You can now ask questions. Type 'quit' to exit, 'help' for options.")
        print("-" * 50)

        # Interactive query loop
        while True:
            try:
                user_input = input("\nYou: ").strip()

                if user_input.lower() in ["quit", "exit", "q"]:
                    print("Goodbye!")
                    break

                if user_input.lower() == "help":
                    print("\nAvailable commands:")
                    print("- Type your question to get a response")
                    print("- 'quit', 'exit', or 'q' to exit")
                    print("- 'help' to show this message")
                    print("- 'model' to show current model info")
                    print("- 'params' to adjust generation parameters")
                    continue

                if user_input.lower() == "model":
                    print(f"\nCurrent model: {self.current_model_name}")
                    print(f"Model device: {self.model.device}")
                    continue

                if user_input.lower() == "params":
                    print("\nCurrent generation parameters:")
                    print("- max_new_tokens: 200")
                    print("- temperature: 1.0")
                    print("- top_p: 1.0")
                    print("(These are fixed for simplicity)")
                    continue

                if not user_input:
                    continue

                # Generate response
                print("\n🤖 Generating response...")
                response = self.query_model(user_input)
                print(f"Assistant: {response}")

            except KeyboardInterrupt:
                print("\n\nGoodbye!")
                break
            except Exception as e:
                print(f"Error: {str(e)}")

# 

class ModelQueryLoraCheckpointInterface(ModelQueryInterface):
    """Interface for querying models with LoRA checkpoints."""

    def load_model(self, model_name: str, device_map: str = "auto", verbose: bool = True,load_in_4bit: bool = False, revision: Optional[str] = None) -> bool:
        """
        Load a model and tokenizer with optional revision.

        Args:
            model_name: HuggingFace model name (name of the adapter repository)
            device_map: Device mapping strategy
            load_in_4bit: Whether to load in 4-bit quantization
            revision: Optional commit hash or branch name

        Returns:
            bool: True if successful, False otherwise
        """
        try:
            # Import PEFT for adapter loading
            from peft import PeftModel
            import json
            from huggingface_hub import hf_hub_download
            
            revision_text = f" (revision: {revision})" if revision else ""
            if verbose:
                print(f"Loading model: {model_name}{revision_text}")
                print("This may take a few minutes for the first time...")

            # Clear GPU memory if available
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                gc.collect()

            # First, download and read adapter_config.json to identify the base model
            try:
                if verbose:
                    print("Downloading adapter config to identify base model...")
                config_file = hf_hub_download(
                    repo_id=model_name,
                    filename="adapter_config.json",
                    revision=revision,
                    token=os.environ.get("HF_TOKEN")
                )
                with open(config_file, 'r') as f:
                    adapter_config = json.load(f)
                
                # Extract base model name from adapter config
                if "base_model_name_or_path" in adapter_config:
                    base_model_name = adapter_config["base_model_name_or_path"]
                    if verbose:
                        print(f"✅ Identified base model from adapter_config: {base_model_name}")
                else:
                    # Try alternative fields that might contain the base model info
                    possible_fields = ["model_name_or_path", "model_id", "_name_or_path", "base_model_name"]
                    for field in possible_fields:
                        if field in adapter_config:
                            base_model_name = adapter_config[field]
                            if verbose:
                                print(f"✅ Identified base model from '{field}' field: {base_model_name}")
                                break
                    else:
                        # If we still don't have a base model name, try to extract it from the model name
                        if "peft" in adapter_config and "peft_type" in adapter_config:
                            # This is a PEFT model, but we don't know the base
                            raise ValueError("Could not determine base model name from adapter config")
                        else:
                            # Not even sure if this is a PEFT model
                            raise ValueError("This doesn't appear to be a valid PEFT adapter configuration")
            except Exception as e:
                print(f"❌ Failed to identify base model: {str(e)}")
                print("Please specify the base model manually using environment variable BASE_MODEL_NAME")
                base_model_name = os.environ.get("BASE_MODEL_NAME")
                if not base_model_name:
                    raise ValueError("Could not determine base model name automatically. Please set BASE_MODEL_NAME environment variable.")
                else:
                    print(f"✅ Using manually specified base model: {base_model_name}")

            # Load tokenizer from the base model
            if verbose:
                print(f"Loading tokenizer from: {base_model_name}...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                base_model_name,
                token=os.environ.get("HF_TOKEN")
            )

            # Load base model
            if verbose:
                print(f"Loading base model: {base_model_name}...")
            base_model = AutoModelForCausalLM.from_pretrained(
                base_model_name,
                device_map=device_map,
                torch_dtype=torch.bfloat16,
                load_in_4bit=load_in_4bit,
                trust_remote_code=True,
                token=os.environ.get("HF_TOKEN")
            )
            
            # Apply LoRA adapter
            if verbose:
                print(f"Applying LoRA adapter from: {model_name}{revision_text}...")
            self.model = PeftModel.from_pretrained(
                base_model,
                model_name,
                revision=revision,
                token=os.environ.get("HF_TOKEN")
            )
            
            self.current_model_name = model_name
            if verbose:
                print(f"✅ Model loaded successfully with adapter!")
                print(f"Model device: {self.model.device}")
            return True

        except Exception as e:
            print(f"❌ Error loading model: {str(e)}")
            return False
        

def main():
    """Main function to run the interactive interface."""
    interface = ModelQueryInterface()
    interface.interactive_mode()


if __name__ == "__main__":
    main()
